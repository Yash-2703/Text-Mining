{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8eda1a-16ec-47cd-9edd-3e493d6d9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip unstall -U spacy\n",
    "#conda insatll -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4215d31b-950d-434f-bd9e-8be6b1829563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.D. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "6 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#create doc object\n",
    "doc = nlp(u'Tesla is looking at buying U.D. startup for $6 million')\n",
    "\n",
    "#print each token seperately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d0373-681b-49c9-8b92-0b3e10acfcd3",
   "metadata": {},
   "source": [
    "# run this command on anaconda prompt\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7a35f8-92c9-44a3-aa5e-15444394f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.D. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Tesla is looking at buying U.D. startup for $6 million')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_, token.dep_)\n",
    "    \n",
    "#Predict syntactic dependencis\n",
    "#Predicting sutactic dependencies involves identifying the grammatical structure if a sentence by determing how different words relate to eachother \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4d2c66-8e31-4768-a5f7-b1ee987f963f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1edcee7ee10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1edcd35bc50>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1edccefd230>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1edcf0e1ed0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1edcf0bdbd0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1edcdde9d20>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd3018a-1f1b-4917-a386-a989b6bf4d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681be545-8cc9-4696-a1b7-c2aa8e675c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "      SPACE dep\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "startups NOUN pobj\n",
      "anymore ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"Tesla isn't      looking into startups anymore.\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9397bea-d6ec-4ee4-9fc7-31b8da0886b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla isn't      looking into startups anymore."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5918d96-f02c-46b5-804b-b6d227eb119a",
   "metadata": {},
   "source": [
    "Notice how isnt has been split into two tokens, Spacy recognizes both the root verb is and the negation attached to it. \n",
    "Noticee also that both the extended whitespace and the period at the end of the sentence assigned that own tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46cf354-8070-4ba7-8ed7-1bb11ad58cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad042cd-7a0f-4d21-a80c-c344a1028b55",
   "metadata": {},
   "source": [
    "POS tagging the next step after splitting the text up into tokens is to assign POS in the above eg. Tesla was recognized to be a proper noun\n",
    "here some stistical modeling is required ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c4980-6fe3-4488-9e18-93808ceb711a",
   "metadata": {},
   "source": [
    "https://spacy.in/usage/linguistic-features #pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe84d43-42ce-40ec-b5f2-3099e9cf629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768d48a4-fdd0-4c39-94bc-8be2ad83b62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32657d56-5410-415b-9f56-0552a705ed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72de7c52-80d5-46b8-9373-550f855cb505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ef4535-e84c-4c2e-91fb-05bda741e04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "print(doc2[4].text)\n",
    "print(doc2[4].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624602f-2b87-4ddd-86b5-bfd182cf54c8",
   "metadata": {},
   "source": [
    "Spans\n",
    "\n",
    "Large Doc object can ve hard to work with at times. A span is a slice of doc object in the form doc[start,stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9641cfb-d3bf-4710-b870-56cb52982d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3= nlp('Although commonly attributed to John Lennon from hus song \"Beautiful Boy\",the phrase \"life is what happes to us while we are making other plans\" was cartoonist allen saunders and published in reader2 Digest ub 1957, when Lennon was 17. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880c4812-6b00-49c3-bc4d-c15709d403ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'life_qoute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m life_quote \u001b[38;5;241m=\u001b[39m doc3[\u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m30\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(life_qoute)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'life_qoute' is not defined"
     ]
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "print(life_qoute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8d591-0f60-451a-9ccd-98b36e81470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c78a0-ef22-4ce7-92c0-07b390fe889a",
   "metadata": {},
   "source": [
    "Sentences\n",
    "\n",
    "Certain tokens inside a Doc object may also recieve a \"Start of sentence\" tag. While the Doesn't immedidately build a list of sentences.\n",
    "these tages enable the generation of sentence segments through doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb71af-3d2f-4a6b-8aa1-a1a12a97cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = doc3= nlp('Although commonly attributed to John Lennon from hus song \"Beautiful Boy\",the phrase \"life is what happes to us while we are making other plans\" was cartoonist allen saunders and published in reader2 Digest ub 1957, when Lennon was 17. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf81066-bfb8-459e-a5b8-22202bff8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb4312-7d57-4518-8c22-105142a6d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a2d7e-e1d0-4d6b-9799-1a5dd648d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring='\"we\\'re moving to L.A.I\"'\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958a669-5450-483e-a609-2622a69d1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc  = nlp(mystring)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, end=\" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64e0e0-5650-4531-8135-d6fb6cc3432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"We're here to help! Sen small-mail, email support@gmail.com  or visit us at http://www.oursite.com!\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, end=\" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7f21f-0e26-41cf-bc96-9a6b05c3f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'A slin NYCC can ride costs $10.30')\n",
    "\n",
    "for t in doc3:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a4af8-0c8a-487c-8ea7-3117523dac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'Lets visit St. Louis in the U.S. next year.')\n",
    "\n",
    "for t in doc4:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907e989-85a9-4429-a03c-bbad22ed010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df96ee-e06d-467b-9bcb-b2edbdf66955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d5ba6-5136-41b3-8f67-2e308bdcdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Downloading the necessary NLTK data (only need to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "def stem_words(word_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_list]\n",
    "    return stemmed_words\n",
    "\n",
    "# Example usage\n",
    "words = ['painful', 'standing', 'abstraction', 'magically']\n",
    "stemmed_words = stem_words(words)\n",
    "print(stemmed_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec29f32-d27c-453a-b0ad-d7c814a6d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords):\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Define stopwords\n",
    "stopwords = [\"is\", \"a\", \"the\", \"an\", \"she\"]\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"an apple is on the table.\"\n",
    "sentence2 = \"She is an engineer.\"\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokens1 = sentence1.split()\n",
    "tokens2 = sentence2.split()\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens1 = remove_stopwords(tokens1, stopwords)\n",
    "filtered_tokens2 = remove_stopwords(tokens2, stopwords)\n",
    "\n",
    "print(filtered_tokens1)  # Output: ['apple', 'on', 'table.']\n",
    "print(filtered_tokens2)  # Output: ['engineer.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e05996-d0e1-484c-bb9a-715129e2779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple|to|build|a|hong|kong|factory|$|6|million|\n",
      "....\n",
      "Apple - ORG - Companies, agencies, institutions, etc.\n",
      "hong kong - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "doc8=nlp(u'Apple to build a hong kong factory $6 million')\n",
    "for token in doc8:\n",
    "    print(token.text,end='|')\n",
    "print('\\n....')\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e40bff-111f-4e68-8562-01741f80ed79",
   "metadata": {},
   "source": [
    "Noun Chunks similar to doc.ents__chunks are another object property. Noun chunks are\n",
    "\"base noun phrases\" - Flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words \n",
    "describing the noun - for example, in sheb wooley's 1958 song. a \"one-eyed, one-handeed, flying, purplr people-earer\" would be long noun chunk.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2129c02e-e6b5-4d03-b72f-da1eb5aeef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auyomonous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9 = nlp(u'Auyomonous cars shift insurance liability toward manufacturers.')\n",
    "\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02452fdb-c31d-498a-bc6c-6e827cd7a6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher isurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10 = nlp(u'Red cars do nott carry higher isurance rates.')\n",
    "\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267ab26-7b4e-4ede-90c8-e7d6e4d25122",
   "metadata": {},
   "source": [
    "Stemming is a somewhat crude method for cataloging related wrds; it essentially chops of letter from the end until stem is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2303edfe-ead6-4971-9729-754176f5b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d137e5-cb72-4b03-8537-19dad2f7df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf9acb7a-df66-4608-859a-7ef5c83b61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'running', 'ran','runs', 'easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2516a9c3-780a-424a-a65b-bed7acf5423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c64daf-da15-4237-af41-a8b689219df0",
   "metadata": {},
   "source": [
    "Not How the stemmer recognized ' runnner' as a noun not a verb form or principle. Aso, the adverbs'easily'\n",
    "and 'fairly' are stemmed to the unusual root 'easli' and 'fairli'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83471d6-6b94-4fa4-ad5c-608749dc6a21",
   "metadata": {},
   "source": [
    "Snowball stemmer it offers a slight improvement ever the original porter stemmer, both in logic and speed.\n",
    "Since nltk uses the name SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93a70d35-7ba3-4d70-817d-5af11d62e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6391605-1f6b-49fe-a253-b291af603018",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'running', 'ran','runs', 'easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f718419-6e45-4406-8bdc-36759ea68e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf5117-33cc-4f68-9fff-784adfbfbce4",
   "metadata": {},
   "source": [
    "Stemming has its Drawbacks . If givern the token saw stemming might sllways return saw whereas lemmatuization \n",
    "would be likely return either see or  saw deoending in whether the use of the token was as a verb or a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e74092a-891d-4b78-aa88-f8cf6105e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "phrase = 'I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e97132-700b-4be5-a17e-a48e640f8050",
   "metadata": {},
   "source": [
    "Here the word meeting appeaes twice - once as a verb and once as a noin and uet the stemmer treats both equality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c068eb-8ecd-4f11-9744-42555a65c635",
   "metadata": {},
   "source": [
    "Lemmatization in contrast tot stemmening lemmatization looks beyond word reduction, and considers a \n",
    "language fill vocabulary to apply a morphologcal analysis to words. Ther lemma of 'was' is 'be; and the lemma of 'mice' is'mouse' .\n",
    "Further, the lemma of 'meeting' might be 'meet' or dependiing on sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b86dff-5554-461e-9d4c-a147cd21dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abd9db88-d6c8-4485-be1b-11e8e44654d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'I am a runner running in a race because I love to run since I ran today')\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t',token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8979f-7f67-464c-9e9b-0556becbad50",
   "metadata": {},
   "source": [
    "Function to display lemmas Since the display above in staggared and hard to read.\n",
    "let's wrtie a function that displaus the info we wnat more nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85a7a663-c3ab-42b4-8fa4-919a5872ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{12}} {token.lemma_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45b974ae-3a0b-4334-969c-8d472f5b27fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903 I\n",
      "am           AUX    10382539506755952630 be\n",
      "meeting      VERB   6880656908171229526 meet\n",
      "him          PRON   1655312771067108281 he\n",
      "tomorrow     NOUN   3573583789758258062 tomorrow\n",
      "at           ADP    11667289587015813222 at\n",
      "the          DET    7425985699627899538 the\n",
      "meeting      NOUN   14798207169164081740 meeting\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'I am meeting him tomorrow at the meeting')\n",
    "\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d91f0b0e-61ab-422e-8afb-a6b0be6dfe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That         PRON   4380130941430378203 that\n",
      "is           AUX    10382539506755952630 be\n",
      "an           DET    15099054000809333061 an\n",
      "enormous     ADJ    17917224542039855524 enormous\n",
      "automobile   NOUN   7211811266693931283 automobile\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'That is an enormous automobile')\n",
    "\n",
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971e4b1-e5e8-4ac7-8d96-9d60425e75d6",
   "metadata": {},
   "source": [
    "Note : lemmatization does not reduce words to their most basic synonym - that is , \n",
    "enormous doesn't become big and sutomobile dosen't become car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c563a30-2225-4656-8a07-079acfa7e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d56dbeba-8b5f-4574-8d53-b5927043f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', 'now', 'seemed', 'yours', 'us', 'few', 'so', 'one', 'whose', 'hereupon', '’ll', 'whole', 'well', 'see', 'are', 'most', 'quite', 'full', 'into', 'several', 'these', 'sometimes', 'others', 'your', 'regarding', 'rather', 'again', 'five', 'had', 'namely', 'hers', 'nobody', 'against', 'besides', 'back', 'below', 'therein', 'everywhere', 'be', 'could', 'whatever', 'top', 'something', 'among', 'out', 'the', 'this', 'say', 'put', 'i', 'their', 'will', 'front', 'whereafter', 'who', 'twenty', 'already', 'enough', 'can', 'ourselves', 'serious', 'would', 'except', 'we', 'him', 'she', 'herein', 'nine', 'since', 'by', 'else', 'itself', 'neither', 'four', 'thru', 'after', 'done', '’s', 'did', \"'m\", 'six', 'without', 'yourselves', '‘s', 'anywhere', 'either', 'being', 'why', 'two', 'seem', 'less', 'am', 'whereupon', 'herself', 'off', 'both', 'forty', 'how', 'someone', 'name', 'indeed', 'hereafter', 'my', 'whenever', 'if', 'might', 'beyond', \"'s\", 'noone', 'wherever', 'whereas', 'ten', 'although', 'from', 'next', 'nowhere', 'using', 'myself', 'never', 'whereby', 'that', 'always', 'what', 'there', 'seems', '’m', 'become', 'though', 'whither', 'which', 'becoming', 'here', 'amount', 'just', 'eleven', 'move', 'keep', 'made', 'various', 'his', 'becomes', 'ever', 'all', 'often', 'it', 'for', 'every', 'otherwise', 'no', 'he', 'nothing', 'twelve', 'side', 'please', 'do', 'any', 'call', 'themselves', 'many', 'nor', 'really', 'anything', 'perhaps', 'once', 'former', \"n't\", 'more', 'very', 'therefore', 'over', 'when', 'doing', 'give', 'toward', 'unless', 'as', 'within', 'where', 'moreover', 'has', 'thus', 'became', 'was', 'per', 'too', 'on', 'formerly', 'thence', 'an', 'mine', 'a', 'at', 'yet', 'n‘t', 'mostly', 'because', 'about', 'latterly', 'above', '’d', 'bottom', 'take', 'go', 'part', 'get', 'under', 'three', 'also', 'least', 'due', 'than', 'anyhow', 'were', 'even', '‘m', 'then', 'together', 'ours', 'none', \"'d\", 'fifty', 'nevertheless', '‘ve', 'beforehand', 'them', 'third', 'upon', 'wherein', 'yourself', 'other', 'afterwards', 'elsewhere', 'make', 'somewhere', 'in', 'not', 'still', 'everyone', 'whether', 'first', 'much', 'to', \"'re\", 'amongst', 'cannot', 'does', 'of', 'onto', 'with', 'last', 'whence', 'whoever', 'before', 'latter', 'those', 'throughout', '‘re', 'sixty', 'ca', 'during', 'along', 'each', 'around', 'however', '’ve', 'up', 'thereafter', 'alone', 'towards', 'down', 'somehow', 'meanwhile', 'our', 'anyone', 'but', 'thereupon', 'they', 'until', 'hence', 'you', 'sometime', 'seeming', 'through', 'himself', 'beside', 'some', 'n’t', 'only', 'her', 'behind', 'empty', 'have', 'show', 'its', 'eight', 'via', 'thereby', 'fifteen', '’re', 'hereby', 'such', '‘d', 'hundred', 'and', 'should', 'is', 'or', 'anyway', '‘ll', 'may', 'whom', 'same', 'almost', 'across', 'another', 'while', 'used', 'between', 'own', 'must', 'me', 'been', 'everything', \"'ll\", \"'ve\", 're'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10a964-9b1d-4ee8-93c7-b178e9ee6f57",
   "metadata": {},
   "source": [
    "# To see if a word is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2cdeb0e-2302-45dc-8960-8af42d7fd43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "242301cd-d2dd-4930-9c8f-ebf7a402a2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea3a0d-84a1-4ea0-ad0d-8f6ed84824b0",
   "metadata": {},
   "source": [
    "# To add a stop word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f32ea6-a502-4d77-adea-0be9f13c1a7e",
   "metadata": {},
   "source": [
    "There may be times when you wish to add a stop word to default sset .Perhaps you  decide that 'btw' should be considered as stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b224f73c-a903-4cdd-89df-a6172c18fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "nlp.vocab['btw'].is_stop =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7368a230-aa09-4ed3-bb5c-2f76ec05f921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f9768b6-699a-47a6-baac-c2a755db0fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d8c0e-8926-46be-899c-f31e57c70d34",
   "metadata": {},
   "source": [
    "# To remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "157085a7-752b-4a85-acca-ba0aa2fc8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "nlp.vocab['btw'].is_stop =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8230e9c6-fd54-4517-b68c-7afad5ad3ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526a6e1-5474-4f8c-b423-a93ac4744c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
